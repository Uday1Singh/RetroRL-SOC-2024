{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "     ---------------------------------------- 0.0/721.7 kB ? eta -:--:--\n",
      "     -- ------------------------------------- 41.0/721.7 kB ? eta -:--:--\n",
      "     -- ---------------------------------- 51.2/721.7 kB 890.4 kB/s eta 0:00:01\n",
      "     --- --------------------------------- 61.4/721.7 kB 469.7 kB/s eta 0:00:02\n",
      "     --- --------------------------------- 61.4/721.7 kB 469.7 kB/s eta 0:00:02\n",
      "     --- --------------------------------- 71.7/721.7 kB 328.6 kB/s eta 0:00:02\n",
      "     ----- ------------------------------ 102.4/721.7 kB 368.6 kB/s eta 0:00:02\n",
      "     ------ ----------------------------- 133.1/721.7 kB 415.1 kB/s eta 0:00:02\n",
      "     --------- -------------------------- 184.3/721.7 kB 484.9 kB/s eta 0:00:02\n",
      "     ---------- ------------------------- 215.0/721.7 kB 486.5 kB/s eta 0:00:02\n",
      "     ----------- ------------------------ 235.5/721.7 kB 497.3 kB/s eta 0:00:01\n",
      "     ------------- ---------------------- 266.2/721.7 kB 528.9 kB/s eta 0:00:01\n",
      "     --------------- -------------------- 317.4/721.7 kB 546.6 kB/s eta 0:00:01\n",
      "     ----------------- ------------------ 348.2/721.7 kB 569.8 kB/s eta 0:00:01\n",
      "     ------------------ ----------------- 378.9/721.7 kB 562.0 kB/s eta 0:00:01\n",
      "     -------------------- --------------- 419.8/721.7 kB 596.2 kB/s eta 0:00:01\n",
      "     ---------------------- ------------- 450.6/721.7 kB 599.9 kB/s eta 0:00:01\n",
      "     ------------------------- ---------- 501.8/721.7 kB 605.3 kB/s eta 0:00:01\n",
      "     -------------------------- --------- 532.5/721.7 kB 619.5 kB/s eta 0:00:01\n",
      "     ----------------------------- ------ 583.7/721.7 kB 632.9 kB/s eta 0:00:01\n",
      "     ------------------------------ ----- 614.4/721.7 kB 644.8 kB/s eta 0:00:01\n",
      "     -------------------------------- --- 655.4/721.7 kB 645.1 kB/s eta 0:00:01\n",
      "     ---------------------------------- - 696.3/721.7 kB 665.5 kB/s eta 0:00:01\n",
      "     ------------------------------------ 721.7/721.7 kB 669.9 kB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\berse\\anaconda3\\envs\\pytorch\\lib\\site-packages (from gym) (1.24.3)\n",
      "Collecting cloudpickle>=1.2.0 (from gym)\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting gym-notices>=0.0.4 (from gym)\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\berse\\anaconda3\\envs\\pytorch\\lib\\site-packages (from gym) (6.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\berse\\anaconda3\\envs\\pytorch\\lib\\site-packages (from importlib-metadata>=4.8.0->gym) (3.11.0)\n",
      "Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml): started\n",
      "  Building wheel for gym (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827636 sha256=b8a145fc991b2d70866f858b92a1a9bc2deabeaddbc981756ed7de1067c9973b\n",
      "  Stored in directory: c:\\users\\berse\\appdata\\local\\pip\\cache\\wheels\\17\\79\\65\\7afedc162d858b02708a3b8f7a6dd5b1000dcd5b0f894f7cc1\n",
      "Successfully built gym\n",
      "Installing collected packages: gym-notices, cloudpickle, gym\n",
      "Successfully installed cloudpickle-3.0.0 gym-0.26.2 gym-notices-0.0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# install gym, numpy, and pygame to run this code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "env = gym.make('CartPole-v1', render_mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0\n",
      "Time step:  0\n",
      "Time step:  1\n",
      "Time step:  2\n",
      "Time step:  3\n",
      "Time step:  4\n",
      "Time step:  5\n",
      "Time step:  6\n",
      "Time step:  7\n",
      "Time step:  8\n",
      "Time step:  9\n",
      "Time step:  10\n",
      "Time step:  11\n",
      "Time step:  12\n",
      "Time step:  13\n",
      "Time step:  14\n",
      "Time step:  15\n",
      "Time step:  16\n",
      "Time step:  17\n",
      "Time step:  18\n",
      "Time step:  19\n",
      "Time step:  20\n",
      "Time step:  21\n",
      "Time step:  22\n",
      "Time step:  23\n",
      "Time step:  24\n",
      "Episode:  1\n",
      "Time step:  0\n",
      "Time step:  1\n",
      "Time step:  2\n",
      "Time step:  3\n",
      "Time step:  4\n",
      "Time step:  5\n",
      "Time step:  6\n",
      "Time step:  7\n",
      "Time step:  8\n",
      "Time step:  9\n",
      "Time step:  10\n",
      "Time step:  11\n",
      "Time step:  12\n",
      "Time step:  13\n",
      "Time step:  14\n",
      "Time step:  15\n",
      "Time step:  16\n",
      "Time step:  17\n",
      "Time step:  18\n",
      "Time step:  19\n",
      "Time step:  20\n",
      "Episode:  2\n",
      "Time step:  0\n",
      "Time step:  1\n",
      "Time step:  2\n",
      "Time step:  3\n",
      "Time step:  4\n",
      "Time step:  5\n",
      "Time step:  6\n",
      "Time step:  7\n",
      "Time step:  8\n",
      "Time step:  9\n",
      "Time step:  10\n",
      "Time step:  11\n",
      "Time step:  12\n",
      "Time step:  13\n",
      "Time step:  14\n",
      "Time step:  15\n",
      "Time step:  16\n",
      "Time step:  17\n",
      "Time step:  18\n",
      "Time step:  19\n",
      "Time step:  20\n",
      "Time step:  21\n",
      "Time step:  22\n",
      "Time step:  23\n",
      "Time step:  24\n",
      "Time step:  25\n",
      "Time step:  26\n",
      "Time step:  27\n",
      "Time step:  28\n",
      "Time step:  29\n",
      "Time step:  30\n",
      "Time step:  31\n",
      "Time step:  32\n",
      "Time step:  33\n",
      "Time step:  34\n",
      "Time step:  35\n",
      "Time step:  36\n",
      "Time step:  37\n",
      "Time step:  38\n",
      "Time step:  39\n",
      "Time step:  40\n",
      "Episode:  3\n",
      "Time step:  0\n",
      "Time step:  1\n",
      "Time step:  2\n",
      "Time step:  3\n",
      "Time step:  4\n",
      "Time step:  5\n",
      "Time step:  6\n",
      "Time step:  7\n",
      "Time step:  8\n",
      "Time step:  9\n",
      "Time step:  10\n",
      "Time step:  11\n",
      "Time step:  12\n",
      "Time step:  13\n",
      "Episode:  4\n",
      "Time step:  0\n",
      "Time step:  1\n",
      "Time step:  2\n",
      "Time step:  3\n",
      "Time step:  4\n",
      "Time step:  5\n",
      "Time step:  6\n",
      "Time step:  7\n",
      "Time step:  8\n",
      "Time step:  9\n",
      "Time step:  10\n",
      "Time step:  11\n",
      "Time step:  12\n",
      "Episode:  5\n",
      "Time step:  0\n",
      "Time step:  1\n",
      "Time step:  2\n",
      "Time step:  3\n",
      "Time step:  4\n",
      "Time step:  5\n",
      "Time step:  6\n",
      "Time step:  7\n",
      "Time step:  8\n",
      "Time step:  9\n",
      "Time step:  10\n",
      "Time step:  11\n",
      "Episode:  6\n",
      "Time step:  0\n",
      "Time step:  1\n",
      "Time step:  2\n",
      "Time step:  3\n",
      "Time step:  4\n",
      "Time step:  5\n",
      "Time step:  6\n",
      "Time step:  7\n",
      "Time step:  8\n",
      "Time step:  9\n",
      "Time step:  10\n",
      "Time step:  11\n",
      "Time step:  12\n",
      "Time step:  13\n",
      "Time step:  14\n",
      "Time step:  15\n",
      "Time step:  16\n",
      "Time step:  17\n",
      "Time step:  18\n",
      "Episode:  7\n",
      "Time step:  0\n",
      "Time step:  1\n",
      "Time step:  2\n",
      "Time step:  3\n",
      "Time step:  4\n",
      "Time step:  5\n",
      "Time step:  6\n",
      "Time step:  7\n",
      "Time step:  8\n",
      "Time step:  9\n",
      "Time step:  10\n",
      "Time step:  11\n",
      "Time step:  12\n",
      "Time step:  13\n",
      "Time step:  14\n",
      "Time step:  15\n",
      "Time step:  16\n",
      "Episode:  8\n",
      "Time step:  0\n",
      "Time step:  1\n",
      "Time step:  2\n",
      "Time step:  3\n",
      "Time step:  4\n",
      "Time step:  5\n",
      "Time step:  6\n",
      "Time step:  7\n",
      "Time step:  8\n",
      "Time step:  9\n",
      "Time step:  10\n",
      "Time step:  11\n",
      "Time step:  12\n",
      "Time step:  13\n",
      "Time step:  14\n",
      "Time step:  15\n",
      "Time step:  16\n",
      "Time step:  17\n",
      "Time step:  18\n",
      "Time step:  19\n",
      "Episode:  9\n",
      "Time step:  0\n",
      "Time step:  1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 23\u001b[0m\n\u001b[0;32m     15\u001b[0m action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Get the observation, reward, done, and info from the environment\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Observation: For the CartPole environment, it is a 4-dimensional vector containing the position of the cart, the velocity of the cart, the angle of the pole, and the angular velocity of the pole\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Reward: For the CartPole environment, it is a scalar value representing the reward for the current time step\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Done: A boolean value indicating whether the episode has ended\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Truncated: A boolean value indicating whether the episode was truncated (meaning that the maximum number of time steps was reached)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Info: A dictionary containing additional information about the environment\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m observation, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m appendedObservations\u001b[38;5;241m.\u001b[39mappend(observation)\n\u001b[0;32m     25\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\gym\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:187\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    184\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:298\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    297\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m--> 298\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Number of episodes and time steps\n",
    "episodeNumber = 10000\n",
    "timeSteps = 100\n",
    "\n",
    "# Loop over episodes\n",
    "for episodeIndex in range(episodeNumber):\n",
    "    # Reset the environment\n",
    "    initial_state = env.reset()\n",
    "    print(\"Episode: \", episodeIndex)\n",
    "    env.render()\n",
    "    appendedObservations = []\n",
    "    for timeIndex in range(timeSteps):\n",
    "        print(\"Time step: \", timeIndex)\n",
    "        # Pick a random action from the action space\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Get the observation, reward, done, and info from the environment\n",
    "        # Observation: For the CartPole environment, it is a 4-dimensional vector containing the position of the cart, the velocity of the cart, the angle of the pole, and the angular velocity of the pole\n",
    "        # Reward: For the CartPole environment, it is a scalar value representing the reward for the current time step\n",
    "        # Done: A boolean value indicating whether the episode has ended\n",
    "        # Truncated: A boolean value indicating whether the episode was truncated (meaning that the maximum number of time steps was reached)\n",
    "        # Info: A dictionary containing additional information about the environment\n",
    "        observation, reward, done, truncated, info = env.step(action)\n",
    "        appendedObservations.append(observation)\n",
    "        env.render()\n",
    "        time.sleep(0.01)\n",
    "        if done:\n",
    "            # If the episode is done, break out of the loop\n",
    "            time.sleep(2)\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interrupt the kernel and run this cell again to close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
